model param: 395B;

hidden_dimension: 4096

seqence_length: 2048

d_key: 64;
d_ffn: 10240;
Flops/seq: 8.7T;


# Architecture overrides
NUM_ENCODER_LAYERS = 24
NUM_DECODER_LAYERS = 24
NUM_HEADS = 64
HEAD_DIM = 64
EMBED_DIM = 4096
MLP_DIM = 10240
GROUP_SIZE = 4096  # Same as Large


batch_size 512;
Nvidia cluster size: 8K;
# MoE overrides
NUM_EXPERTS = 128
# Every other layer has an MoE sublayer.
NUM_ENCODER_SPARSE_LAYERS = 12
NUM_DECODER_SPARSE_LAYERS = 12
TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
EVAL_EXPERT_CAPACITY_FACTOR = 4.   # Larger than Large
moe_layers.MoeLayer.min_expert_capacity = 2   # Smaller than Large

# Switch Transformer XXL uses GEGLU activations.
dense.MlpBlock.activations = ('gelu', 'linear')
expert/dense.MlpBlock.activations = ('gelu', 'linear')

# Switch Transformer XXL uses a separate output logits layer.
moe_architecture.SparseDecoder.output_logits_factory = @output_logits/dense.DenseGenera


